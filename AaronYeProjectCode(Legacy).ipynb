{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Please Note: This is not the final code utilized in the paper!\n",
        "This is \"legacy code\" that I am showing as it is part of my innitial steps for this project"
      ],
      "metadata": {
        "id": "k8uF08Iyr6Gt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td1Ho0SipHP7"
      },
      "outputs": [],
      "source": [
        "#Initial steps used alot of built in keras functionality as a \"sanity check\"\n",
        "import numpy as np\n",
        "import keras as keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Dropout , Activation\n",
        "from keras.layers import LSTM , Embedding\n",
        "from keras.layers import MaxPooling1D , GlobalMaxPooling1D,Conv1D , Flatten\n",
        "from keras.datasets import imdb\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Take any datasets with a frequency atmost reaching the \"top_words\" limit\n",
        "top_words = 8000\n",
        "(X_train , y_train), (X_test , y_test) = imdb.load_data(num_words = top_words)\n",
        "\n",
        "#Debug code that is used to convert self created string into arrays useable for testing\n",
        "print(X_test[0])\n",
        "imdbDict = keras.datasets.imdb.get_word_index()\n",
        "\n",
        "inputString = \"I think the movie was really good the shots were stunning and the actors did a good job portraying the characters would recommend this is definitely one of the directors better works i am stunned at the amazement\"\n",
        "inputArray = inputString.split()\n",
        "convertedArray = []\n",
        "for x in inputArray:\n",
        "  if (x in imdbDict):\n",
        "    convertedArray.append(imdbDict[x])\n",
        "  else:\n",
        "    print(\"<\",x, \"> was not found in the imdb dictionary\")\n",
        "\n",
        "print(convertedArray)\n",
        "\n"
      ],
      "metadata": {
        "id": "caUu6cgCsTar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The actual model, first pad the sequence, then perform an architecture similar\n",
        "#to that of an AlexNet except only using 1D convolution and pooling layers\n",
        "\n",
        "max_words = 450\n",
        "print(X_train.shape)\n",
        "x_train = sequence.pad_sequences(X_train , maxlen = max_words)\n",
        "x_test = sequence.pad_sequences(X_test , maxlen = max_words)\n",
        "print(x_train.shape)\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words , 32 , input_length = max_words))\n",
        "\n",
        "model.add(Conv1D(128 , 5 , padding = 'same',activation = 'relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "\n",
        "model.add(Conv1D(128 , 5 , padding = 'same',activation = 'relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "\n",
        "model.add(Conv1D(128 , 5 , padding = 'same',activation = 'relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(120, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "cxU17ajWsfKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting and evaluating\n",
        "\n",
        "model.fit(x_train , y_train , validation_data= (x_test , y_test) ,batch_size = 128, epochs = 10 , verbose = 2)\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "#Additional Evaluation Metrics\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print('Accuracy:', accuracy)\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1-score:', f1)\n",
        "\n",
        "#TEsting of self created sentences\n",
        "compiledScore = model.predict(sequence.pad_sequences([convertedArray] , maxlen = max_words))[0][0]\n",
        "\n",
        "print(compiledScore)\n",
        "if compiledScore < 0.5:\n",
        "  compiledText = \"Negative\"\n",
        "else:\n",
        "  compiledText = \"Positive\"\n",
        "\n",
        "print(\"Review made:\", inputString, \", Compiled Score:\", compiledText)"
      ],
      "metadata": {
        "id": "2zpdWlROsk_y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}